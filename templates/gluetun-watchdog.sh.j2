#!/bin/bash
# Gluetun VPN watchdog - detects crash loops and does a full container restart
# Managed by Ansible - do not edit manually

set -uo pipefail

LOG_TAG="gluetun-watchdog"
STATE_DIR="/var/lib/gluetun-watchdog"
FAILURE_COUNT_FILE="${STATE_DIR}/failure_count"
PORTFWD_FAILURE_COUNT_FILE="${STATE_DIR}/portfwd_failure_count"
RESTART_LOG_FILE="${STATE_DIR}/restart_log"

# Configuration (from Ansible variables)
MAX_FAILURES="{{ gluetun_watchdog_max_failures | default(3) }}"
MAX_PORTFWD_FAILURES="{{ gluetun_watchdog_max_portfwd_failures | default(5) }}"
MAX_RESTARTS="{{ gluetun_watchdog_max_restarts | default(5) }}"
RESTART_WINDOW="{{ gluetun_watchdog_restart_window | default(3600) }}"
COMPOSE_DIR="{{ gluetun_watchdog_compose_dir | default('/opt/media-stack') }}"
CONTAINER_NAME="{{ gluetun_watchdog_container | default('gluetun') }}"
# Containers sharing gluetun's network namespace (must be recreated together)
DEPENDENT_CONTAINERS="{{ gluetun_watchdog_dependent_containers | default('qbittorrent') }}"

mkdir -p "$STATE_DIR"

log() { logger -t "$LOG_TAG" "$*"; }

send_notification() {
    local title="$1"
    local body="${2:-}"
    local type="${3:-warning}"
    curl -sf -o /dev/null \
      -X POST "{{ apprise_endpoint }}/notify/{{ apprise_token }}" \
      --form-string "title=${title}" \
      --form-string "body=${body}" \
      --form-string "tag={{ gluetun_watchdog_notify_tag | default('push-quiet') }}" \
      --form-string "type=${type}" \
      --max-time 10 \
      --retry 2 \
      --retry-delay 3 2>/dev/null || true
}

# ---------------------------------------------------------------------------
# Failure counting (persistent across runs)
# ---------------------------------------------------------------------------

get_failure_count() {
    [[ -f "$FAILURE_COUNT_FILE" ]] && cat "$FAILURE_COUNT_FILE" 2>/dev/null || echo 0
}

set_failure_count() {
    echo "$1" > "$FAILURE_COUNT_FILE"
}

# ---------------------------------------------------------------------------
# Restart rate limiting (prevent infinite restart loops)
# ---------------------------------------------------------------------------

get_recent_restart_count() {
    if [[ ! -f "$RESTART_LOG_FILE" ]]; then
        echo 0
        return
    fi
    local cutoff
    cutoff=$(( $(date +%s) - RESTART_WINDOW ))
    awk -v cutoff="$cutoff" '$1 > cutoff' "$RESTART_LOG_FILE" 2>/dev/null | wc -l
}

record_restart() {
    echo "$(date +%s)" >> "$RESTART_LOG_FILE"
    # Prune entries older than the window
    local cutoff
    cutoff=$(( $(date +%s) - RESTART_WINDOW ))
    if [[ -f "$RESTART_LOG_FILE" ]]; then
        awk -v cutoff="$cutoff" '$1 > cutoff' "$RESTART_LOG_FILE" > "${RESTART_LOG_FILE}.tmp"
        mv "${RESTART_LOG_FILE}.tmp" "$RESTART_LOG_FILE"
    fi
}

# ---------------------------------------------------------------------------
# Health check
# ---------------------------------------------------------------------------

check_gluetun_health() {
    # Check if container exists and is running
    if ! docker inspect "$CONTAINER_NAME" &>/dev/null; then
        log "Container $CONTAINER_NAME does not exist - skipping"
        return 2  # Special code: container doesn't exist, nothing to do
    fi

    local state
    state=$(docker inspect --format {% raw %}'{{.State.Status}}'{% endraw %} "$CONTAINER_NAME" 2>/dev/null)
    if [[ "$state" != "running" ]]; then
        log "Container $CONTAINER_NAME is not running (state: $state)"
        return 1
    fi

    # Check Docker healthcheck status (Gluetun defines its own healthcheck)
    local health
    health=$(docker inspect --format {% raw %}'{{if .State.Health}}{{.State.Health.Status}}{{else}}none{{end}}'{% endraw %} "$CONTAINER_NAME" 2>/dev/null)

    case "$health" in
        healthy)
            return 0
            ;;
        starting)
            # Give it time to finish startup
            log "Container is starting up - waiting"
            return 1
            ;;
        unhealthy)
            log "Docker healthcheck reports unhealthy"
            return 1
            ;;
        none)
            # No Docker healthcheck defined - test connectivity directly
            if docker exec "$CONTAINER_NAME" wget -qO- --timeout=5 http://ipinfo.io/ip &>/dev/null; then
                return 0
            fi
            log "Direct connectivity test failed"
            return 1
            ;;
    esac

    return 1
}

# ---------------------------------------------------------------------------
# Port forwarding check
# ---------------------------------------------------------------------------

get_portfwd_failure_count() {
    [[ -f "$PORTFWD_FAILURE_COUNT_FILE" ]] && cat "$PORTFWD_FAILURE_COUNT_FILE" 2>/dev/null || echo 0
}

set_portfwd_failure_count() {
    echo "$1" > "$PORTFWD_FAILURE_COUNT_FILE"
}

check_port_forwarding() {
    # Query Gluetun's control API for the forwarded port
    local response
    response=$(docker exec "$CONTAINER_NAME" wget -qO- --timeout=5 http://127.0.0.1:8000/v1/portforward 2>/dev/null) || return 1

    local port
    port=$(echo "$response" | grep -o '"port":[0-9]*' | grep -o '[0-9]*')

    if [[ -z "$port" || "$port" -eq 0 ]]; then
        return 1
    fi

    return 0
}

# ---------------------------------------------------------------------------
# Restart logic
# ---------------------------------------------------------------------------

restart_gluetun() {
    local recent_restarts
    recent_restarts=$(get_recent_restart_count)

    if [[ "$recent_restarts" -ge "$MAX_RESTARTS" ]]; then
        log "ERROR: $MAX_RESTARTS restarts within $(( RESTART_WINDOW / 60 ))m window - manual intervention required"
        send_notification \
            "Gluetun watchdog: manual intervention required" \
            "$MAX_RESTARTS restarts in $(( RESTART_WINDOW / 60 ))m on $(hostname). VPN is down." \
            "failure"
        return 1
    fi

    log "Restarting $CONTAINER_NAME + dependents (restart $((recent_restarts + 1))/$MAX_RESTARTS within window)..."
    record_restart

    # Force-recreate to destroy the network namespace and clear stale tun0 routes.
    # Dependent containers (e.g. qbittorrent) share gluetun's namespace and must
    # be recreated together, otherwise they're orphaned on the old namespace.
    # shellcheck disable=SC2086
    cd "$COMPOSE_DIR" && docker compose up -d --force-recreate "$CONTAINER_NAME" $DEPENDENT_CONTAINERS 2>&1 | while read -r line; do
        log "$line"
    done

    # Wait for container to stabilize
    sleep 30

    if check_gluetun_health; then
        log "Container recovered after force-recreate"
        send_notification \
            "Gluetun watchdog: VPN recovered" \
            "Force-recreated $CONTAINER_NAME + dependents on $(hostname). VPN is back up." \
            "info"
        return 0
    else
        log "Container still unhealthy after force-recreate - will retry next cycle"
        return 1
    fi
}

# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

main() {
    local health_result=0
    check_gluetun_health || health_result=$?

    # Container doesn't exist - nothing to monitor
    if [[ "$health_result" -eq 2 ]]; then
        set_failure_count 0
        set_portfwd_failure_count 0
        return 0
    fi

    # Healthy - check port forwarding too
    if [[ "$health_result" -eq 0 ]]; then
        local prev_failures
        prev_failures=$(get_failure_count)
        if [[ "$prev_failures" -gt 0 ]]; then
            log "VPN healthy (was at $prev_failures consecutive failures)"
        fi
        set_failure_count 0

        # Port forwarding check (only meaningful when VPN is healthy)
        if check_port_forwarding; then
            local prev_pf_failures
            prev_pf_failures=$(get_portfwd_failure_count)
            if [[ "$prev_pf_failures" -gt 0 ]]; then
                log "Port forwarding restored (was at $prev_pf_failures consecutive failures)"
            fi
            set_portfwd_failure_count 0
        else
            local pf_failures
            pf_failures=$(get_portfwd_failure_count)
            pf_failures=$((pf_failures + 1))
            set_portfwd_failure_count "$pf_failures"

            log "Port forwarding down ($pf_failures/$MAX_PORTFWD_FAILURES consecutive failures)"

            if [[ "$pf_failures" -ge "$MAX_PORTFWD_FAILURES" ]]; then
                log "Port forwarding stuck - force-recreating to get new port assignment"
                send_notification \
                    "Gluetun watchdog: port forwarding lost" \
                    "Port forwarding has been down for $pf_failures checks on $(hostname). Restarting Gluetun to get a new port." \
                    "warning"
                restart_gluetun
                set_portfwd_failure_count 0
            fi
        fi
        return 0
    fi

    # Unhealthy - increment failure counter, reset port fwd counter
    local failures
    failures=$(get_failure_count)
    failures=$((failures + 1))
    set_failure_count "$failures"
    set_portfwd_failure_count 0

    log "VPN unhealthy ($failures/$MAX_FAILURES consecutive failures)"

    if [[ "$failures" -ge "$MAX_FAILURES" ]]; then
        restart_gluetun
        set_failure_count 0
    fi
}

main "$@"
